# Python Codebase Audit Heuristics

## Table of Contents

1. [Introduction](#introduction)
2. [Systematic Audit Methodology](#systematic-audit-methodology)
3. [Python Best Practices & Style Guidelines](#python-best-practices--style-guidelines)
4. [LLM & AI Code Generation Error Patterns](#llm--ai-code-generation-error-patterns)
5. [Code Review Techniques](#code-review-techniques)
6. [Static Analysis Tools](#static-analysis-tools)
7. [Python Anti-Patterns](#python-anti-patterns)
8. [Security Vulnerabilities](#security-vulnerabilities)
9. [Testing Best Practices](#testing-best-practices)
10. [Performance Optimization](#performance-optimization)
11. [Concurrency & Async Patterns](#concurrency--async-patterns)
12. [Documentation Standards](#documentation-standards)
13. [Database & Type System Pitfalls](#database--type-system-pitfalls)
14. [Complete Audit Checklist](#complete-audit-checklist)

---

## Introduction

This document provides comprehensive heuristics for auditing Python codebases, with particular attention to code patterns generated by LLMs and coding agents. The goal is to identify issues across multiple dimensions: style, correctness, security, performance, and maintainability.

### Guiding Principles

- **Consistency over personal preference**: Style consistency within a project is paramount
- **Explicit is better than implicit**: Code should be clear and self-documenting
- **Fail fast**: Errors should be caught early through static analysis and testing
- **Security by default**: Assume untrusted input and validate rigorously
- **Profile before optimizing**: Avoid premature optimization without data

---

## Systematic Audit Methodology

**Based on real-world audit experience from GeistFabrik stats command implementation (January 2025)**

This section captures lessons learned from auditing a complete feature implementation, revealing systematic oversights and effective detection strategies.

### The Five Root Causes of Overlooked Issues

Analysis of a real stats command implementation revealed five core reasons why issues were initially missed:

#### 1. Speed Over Thoroughness
**Problem**: Prioritizing task completion speed over comprehensive implementation
**Manifestation**: Marking todos complete without verification, skipping difficult parts
**Solution**: Build verification into the definition of "done"

```python
# BAD: Claiming complete without running
def compute_metrics(embeddings):
    # Implementation here...
    pass
# Mark as done ✓ (Never ran the code!)

# GOOD: Verification is part of completion
def compute_metrics(embeddings):
    # Implementation here...
    pass

# Before marking complete:
# 1. Run the function with real data
# 2. Check output format matches spec
# 3. Verify edge cases (empty input, single item, etc.)
```

#### 2. Implementation Without Validation
**Problem**: Writing code but never executing it to verify behavior
**Critical insight**: **Assume your code doesn't work until you've proven it does**

**The Validation Checklist**:
- [ ] Run the code with typical inputs
- [ ] Run the code with edge case inputs (empty, single, max)
- [ ] Check output manually (not just "no error")
- [ ] Verify against specification requirements
- [ ] Test error conditions

```python
# BAD: Implementation without verification
def get_temporal_drift(self, current_date: str) -> Dict[str, Any]:
    # 50 lines of implementation
    return drift_analysis
# Ship it! ❌

# GOOD: Implementation with systematic validation
def get_temporal_drift(self, current_date: str) -> Dict[str, Any]:
    # 50 lines of implementation
    return drift_analysis

# Validation (before claiming complete):
vault = load_test_vault()
result = vault.get_temporal_drift("2025-01-15")
print(f"Keys: {result.keys()}")  # Check structure
print(f"Drift: {result['average_drift']}")  # Check values make sense
assert "high_drift_notes" in result  # Verify required fields
```

#### 3. Satisficing (Good Enough Syndrome)
**Problem**: Implementing "good enough" instead of spec-complete
**Manifestation**: Skipping optional features, simplified implementations, incomplete error handling

**Detection**:
```bash
# Compare implementation against spec systematically
diff <(grep "^- \[x\]" FEATURE_SPEC.md | wc -l) \
     <(grep "def.*:" implementation.py | wc -l)
```

**Example**: Stats command initially had:
- Basic metrics ✓
- Advanced metrics ✗ (skipped TwoNN, Vendi Score, IsoScore)
- Temporal drift ✗ (claimed too complex)
- Verbose mode ✗ (partial implementation)

**Solution**: Treat specs as contracts, not suggestions.

#### 4. Avoiding Complexity
**Problem**: Skipping the hardest parts, implementing easy features first
**Psychological pattern**: Dopamine from completing easy tasks, dread from complex ones

**The Complexity-First Rule**:
> Implement the hardest 20% first. If you can't do the hard parts, the easy parts don't matter.

```python
# BAD: Implementing easy parts first
# ✓ Basic stats collection (easy)
# ✓ Simple formatters (easy)
# ✗ Temporal drift analysis (hard - "too complex")
# ✗ Advanced embedding metrics (hard - "requires dependencies")

# GOOD: Tackle complex parts first
# ✓ Temporal drift with Procrustes alignment
# ✓ TwoNN intrinsic dimensionality
# ✓ Vendi Score implementation
# ✓ Then simple stats collection
# ✓ Then formatters
```

#### 5. No Testing Discipline
**Problem**: Treating tests as optional post-hoc verification instead of part of implementation

**The Test-First Mindset**:
- Tests aren't "done after implementation"
- Tests are HOW you verify implementation
- No tests = No confidence = Not done

```python
# BAD: Implementation without tests
def parse_date(date_str: str) -> datetime:
    # Complex parsing logic
    return parsed_date
# "I'll write tests later" ❌

# GOOD: Test-driven verification
def test_parse_date_iso_format():
    assert parse_date("2025-01-15") == datetime(2025, 1, 15)

def test_parse_date_invalid():
    with pytest.raises(ValueError):
        parse_date("not-a-date")

def test_parse_date_edge_cases():
    assert parse_date("2025-12-31") == datetime(2025, 12, 31)
    assert parse_date("2000-01-01") == datetime(2000, 1, 1)
```

### Systematic Audit Process

A three-phase approach proven effective for catching implementation gaps:

#### Phase 1: Completeness Check
**Goal**: Verify every spec requirement is implemented

```markdown
# Create audit document
## Section 1: Vault Overview
**Spec Requirements:**
- Vault path ✓
- Database size ✓
- Last sync ✓
- Configuration path ✓
- Vector backend ✗ (MISSING)

## Section 2: Embedding Metrics
**Spec Requirements:**
- Intrinsic dimensionality ✗ (MISSING)
- Vendi Score ✗ (MISSING)
...
```

**Process**:
1. List every requirement from spec
2. Check implementation for each requirement
3. Mark ✓ (implemented) or ✗ (missing/incomplete)
4. Don't accept partial credit

#### Phase 2: Correctness Check
**Goal**: Verify implementation behavior matches spec

**SQL Query Audit**:
```sql
-- Check every SQL query for:
-- 1. Correct JOINs
SELECT s.date, COUNT(*) FROM sessions s
JOIN session_embeddings se ON s.date = se.session_id  -- ❌ WRONG
-- Should be: s.session_id = se.session_id

-- 2. Efficient queries
SELECT COUNT(DISTINCT source_path) FROM links  -- ❌ INEFFICIENT
-- Should be: SELECT source_path FROM links GROUP BY source_path

-- 3. Correct aggregations
SELECT COUNT(*) as n_gaps FROM embeddings WHERE cluster = -1  -- ❌ WRONG TABLE
-- Should be: Use clustering results, not embeddings table
```

**Data Flow Audit**:
```python
# Trace data from input to output
embedding_metrics = compute_metrics()  # Returns Dict[str, Any]
cached = _cache_metrics(metrics)       # Stores to SQLite
loaded = _load_cached_metrics()        # Reads from SQLite
formatted = format_metrics(loaded)     # Formats for display

# Check at each step:
# - Types preserved? (int stays int, not becomes blob)
# - All fields present? (dimension, n_notes added when loading cache)
# - Values make sense? (negative percentages, impossible counts)
```

#### Phase 3: Edge Case Testing
**Goal**: Find bugs through actual execution

**The Edge Case Matrix**:
```python
# Test with:
# - Empty inputs (0 notes, 0 embeddings)
# - Single item (1 note, 1 embedding)
# - Large inputs (1000+ notes)
# - Missing data (no sessions, no cache)
# - Invalid data (corrupted SQLite, malformed embeddings)
# - Type boundaries (numpy types, None values, blobs)

def test_edge_cases():
    # Empty vault
    stats = collect_stats(empty_vault)
    assert stats["notes"]["total"] == 0
    assert stats["embeddings"] is None  # Not crash

    # Single note
    stats = collect_stats(single_note_vault)
    assert stats["graph"]["density"] is not None  # Handle division by zero

    # No cache
    metrics = compute_metrics(embeddings, force_recompute=True)
    assert "dimension" in metrics  # Don't rely on cache
```

### Common Bugs Found Through Systematic Audits

Based on real findings from stats command audit:

#### 1. SQLite Type Serialization Issues
**Problem**: NumPy integers stored as blobs instead of INTEGER

```python
# BAD: Direct insertion of numpy types
metrics = {"n_clusters": np.int64(5), "n_gaps": np.int64(8)}
cursor.execute("INSERT INTO metrics VALUES (?, ?)",
               (metrics["n_clusters"], metrics["n_gaps"]))
# Stored as blobs: b'\x05\x00\x00\x00\x00\x00\x00\x00'

# GOOD: Explicit type conversion
def to_python_type(val: Any) -> Any:
    if isinstance(val, (np.integer, np.int64, np.int32)):
        return int(val)
    if isinstance(val, (np.floating, np.float64, np.float32)):
        return float(val)
    return val

metrics = {"n_clusters": to_python_type(5), "n_gaps": to_python_type(8)}
cursor.execute("INSERT INTO metrics VALUES (?, ?)",
               (metrics["n_clusters"], metrics["n_gaps"]))
```

**Detection**: Manual inspection of SQLite data
```bash
sqlite3 vault.db "SELECT typeof(n_gaps), n_gaps FROM metrics LIMIT 1"
# Bad: blob|
# Good: integer|8
```

#### 2. Missing Fields in Cached Data
**Problem**: Cache stores subset of fields, missing others on retrieval

```python
# BAD: Returning cached data as-is
def compute_metrics(embeddings, force_recompute=False):
    if not force_recompute:
        cached = load_cache()
        if cached:
            return cached  # ❌ Missing dimension, n_notes!

    # Compute fresh metrics
    return full_metrics

# GOOD: Always populate dynamic fields
def compute_metrics(embeddings, force_recompute=False):
    if not force_recompute:
        cached = load_cache()
        if cached:
            # Always include fields that can change
            cached["dimension"] = embeddings.shape[1]
            cached["n_notes"] = len(embeddings)
            return cached

    return full_metrics
```

#### 3. SQL Query Correctness
**Problems found**:
- Wrong JOIN columns (string vs integer)
- Inefficient queries (unnecessary COUNT DISTINCT)
- Wrong table references

```python
# BAD: Joining on wrong columns
"""
SELECT s.date, COUNT(*) FROM sessions s
JOIN session_embeddings se ON s.date = se.session_id
"""
# ❌ Comparing TEXT (date) with INTEGER (session_id)

# GOOD: Join on correct foreign key
"""
SELECT s.date, COUNT(*) FROM sessions s
JOIN session_embeddings se ON s.session_id = se.session_id
"""
```

#### 4. Test Data Mismatches
**Problem**: Tests expect different field names than implementation

```python
# Implementation uses:
stats = {"notes": {"total": 10, "orphans": 2}}

# Tests expect:
assert stats["notes"]["total_notes"] == 10  # ❌ KeyError!

# Solution: Run tests to find mismatches (don't assume tests match)
```

### The Validation Workflow

**Critical lesson**: Use project-specific validation scripts, not ad-hoc commands.

```bash
# ❌ BAD: Custom variations of CI checks
mypy src/ --ignore-missing-imports  # Different from CI!
pytest tests/unit -k "stats"         # Missing integration tests!

# ✅ GOOD: Use exact CI validation
./scripts/validate.sh  # Runs EXACT same checks as CI
```

**Why this matters**:
- PR #30 failed CI despite local "testing" because commands differed
- Validation script guarantees: local pass = CI pass
- Saves hours of CI failure debugging

**The validate.sh pattern**:
```bash
#!/bin/bash
set -e  # Exit on first failure

echo "Running EXACT CI checks..."

# 1. Linting (exact CI command)
ruff check src/ tests/

# 2. Type checking (exact CI command)
mypy src/ --strict

# 3. Database validation (project-specific)
python scripts/detect_unused_tables.py

# 4. Unit tests (exact CI command)
pytest tests/unit -v

# 5. Integration tests (exact CI command)
pytest tests/integration -v -m "not slow"

echo "✅ All checks passed. Safe to push."
```

### Self-Audit Questions

Before claiming a feature is complete, ask:

1. **Completeness**: Did I implement every requirement from the spec?
2. **Validation**: Have I actually run this code with real data?
3. **Testing**: Do tests cover the critical paths and edge cases?
4. **Correctness**: Have I verified the output is correct, not just error-free?
5. **Complexity**: Did I implement the hard parts, or just the easy ones?
6. **Edge cases**: What happens with empty input? Single item? Maximum size?
7. **Type safety**: Are numpy types converted for SQLite? Are None values handled?
8. **SQL correctness**: Are JOINs correct? Are queries efficient?
9. **Cache consistency**: Does cached data match fresh data structure?
10. **Documentation**: Are specs updated to reflect implementation status?

### Key Takeaways

1. **Never trust code you haven't run** - Implementation without validation is incomplete
2. **Specs are contracts, not suggestions** - Partial implementation is failure
3. **Do the hard parts first** - If you can't do them, the easy parts don't matter
4. **Tests are verification, not documentation** - Write tests to prove correctness
5. **Manual testing catches what unit tests miss** - Actually use the feature
6. **Systematic audits find oversights** - Completeness, correctness, edge cases
7. **Use project validation scripts** - Don't improvise CI commands
8. **Type conversions matter** - NumPy → Python, especially for SQLite
9. **Cache consistency requires discipline** - Cached data must match fresh data
10. **Document as you implement** - Update specs when features are done

---

## Python Best Practices & Style Guidelines

### PEP 8 - Style Guide for Python Code

PEP 8 is the official style guide created by Guido van Rossum, Barry Warsaw, and Nick Coghlan. Key guidelines:

#### Indentation & Line Length

- **Indentation**: Use 4 spaces per indentation level (never tabs)
- **Line length**: Maximum 79 characters for code, 72 for comments/docstrings
- **Extended line length**: Some teams accept up to 99 characters for code

#### Naming Conventions

- **Modules**: `lowercase_with_underscores.py`
- **Classes**: `PascalCase` or `CapWords`
- **Functions/Methods**: `lowercase_with_underscores()`
- **Constants**: `UPPERCASE_WITH_UNDERSCORES`
- **Private attributes**: `_leading_underscore`
- **Name mangling**: `__double_leading_underscore` (for avoiding name clashes in subclasses)

#### Whitespace & Layout

- Avoid extraneous whitespace in brackets, before commas, or around keyword arguments
- Use blank lines to separate logical sections
- Two blank lines between top-level definitions
- One blank line between method definitions within a class

#### Import Organisation

```python
# Standard library imports
import os
import sys

# Third-party imports
import numpy as np
import pandas as pd

# Local application imports
from mypackage import mymodule
```

#### Enforcement Tools

- **Black**: Opinionated autoformatter, enforces PEP 8 compliance automatically
- **Flake8**: Linter that checks PEP 8 compliance and catches bad patterns
- **autopep8**: Automatically formats code to conform to PEP 8
- **IDE integration**: PyCharm, VS Code, and other IDEs have built-in PEP 8 checking

### PEP 484 - Type Hints

Introduced in Python 3.5, type hints provide static type annotations without making Python enforce types at runtime.

#### Key Concepts

- Python remains dynamically typed; type hints are **optional**
- Type hints serve as documentation and enable static analysis
- Type checkers like mypy operate offline as "very powerful linters"
- Annotations should be kept simple for static analysis tools

#### Best Practices

```python
from typing import List, Dict, Optional, Union, Callable

def greet(name: str) -> str:
    return f"Hello, {name}"

def process_items(items: List[str]) -> Dict[str, int]:
    return {item: len(item) for item in items}

def find_user(user_id: int) -> Optional[User]:
    # Returns User or None
    pass

def combine(x: Union[int, str]) -> str:
    return str(x)
```

#### Benefits

- Improved code readability and self-documentation
- Better IDE support (autocomplete, refactoring, linting)
- Early bug detection through static analysis
- Enhanced scalability for large projects

#### Type Checking with Mypy

- Mypy warns about incorrect type usage
- Catches bugs without running code
- Install: `pip install mypy`
- Run: `mypy your_module.py`

---

## LLM & AI Code Generation Error Patterns

AI-generated code exhibits systematic error patterns that human reviewers should watch for.

### Semantic Errors (Logical Mistakes)

#### 1. Condition Errors

**Problem**: Missing necessary conditions or incorrectly formulated conditionals

```python
# BAD: Missing edge case handling
def divide(a, b):
    return a / b  # No check for b == 0

# GOOD: Proper condition checking
def divide(a, b):
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b
```

#### 2. Constant Value Errors

**Problem**: Incorrect constant values in function arguments or assignments

```python
# BAD: Wrong default value
def retry_connection(max_attempts=0):  # Should be > 0
    for i in range(max_attempts):
        try:
            connect()
        except ConnectionError:
            continue

# GOOD: Sensible default
def retry_connection(max_attempts=3):
    for i in range(max_attempts):
        try:
            connect()
        except ConnectionError:
            continue
```

#### 3. Operation/Calculation Errors

**Problem**: Mistakes in mathematical or logical operations

```python
# BAD: Wrong calculation
def calculate_percentage(part, total):
    return (part / total)  # Missing * 100

# GOOD: Correct calculation
def calculate_percentage(part, total):
    return (part / total) * 100
```

#### 4. Incomplete Code/Missing Steps

**Problem**: Absence of crucial steps needed to complete the task

```python
# BAD: Incomplete implementation
def save_user_data(user_data):
    # Opens file but never writes or closes
    f = open("users.json", "w")

# GOOD: Complete implementation
def save_user_data(user_data):
    with open("users.json", "w") as f:
        json.dump(user_data, f)
```

#### 5. Memory Errors

**Problem**: Infinite loops or recursions that never terminate

```python
# BAD: Infinite recursion
def factorial(n):
    return n * factorial(n - 1)  # No base case

# GOOD: Proper recursion with base case
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)
```

#### 6. Garbage Code

**Problem**: Unnecessary code that doesn't contribute to functionality

```python
# BAD: Redundant code
def process_data(data):
    temp = data
    result = temp
    final = result
    return final

# GOOD: Clean and direct
def process_data(data):
    return data
```

### Syntactic and Runtime Errors

#### 1. API Misuse

**Problem**: Incorrect use of library calls, APIs, and operators

```python
# BAD: Wrong API usage
import requests
response = requests.get("https://api.example.com")
data = response.json  # Missing parentheses

# GOOD: Correct API usage
import requests
response = requests.get("https://api.example.com")
data = response.json()
```

#### 2. Library Import Errors

**Problem**: Missing import statements or incorrect import levels

```python
# BAD: Missing import
def calculate_mean(numbers):
    return np.mean(numbers)  # NameError: np not defined

# GOOD: Proper import
import numpy as np

def calculate_mean(numbers):
    return np.mean(numbers)
```

#### 3. Definition Missing

**Problem**: Omitted definitions of commonly used variables or functions

```python
# BAD: Undefined variable
def process():
    return result  # result never defined

# GOOD: Defined variable
def process():
    result = perform_calculation()
    return result
```

#### 4. Hallucinations

**Problem**: Code that is syntactically plausible but factually incorrect

```python
# BAD: Hallucinated package (5.2% hallucination rate for Python packages)
from data_processor_ultra import clean_data  # Package doesn't exist

# BAD: Hallucinated method
text = "hello"
text.to_uppercase()  # Method doesn't exist (should be upper())

# GOOD: Real methods
text = "hello"
text.upper()
```

**Statistics**: Research found 5.2% hallucinated packages in Python (21.7% in JavaScript) across 576,000 code samples from 16 LLMs.

#### 5. Input/Output Format Errors

**Problem**: Incorrect order of inputs/outputs or wrong precision

```python
# BAD: Wrong argument order
def create_user(email, username, password):
    pass

create_user("pass123", "john_doe", "john@example.com")  # Wrong order

# GOOD: Use keyword arguments or correct order
create_user(username="john_doe", email="john@example.com", password="pass123")
```

#### 6. Language-Specific Confusion

**Problem**: Mixing functionality from different languages

```python
# BAD: Using Java-style regex split in Python
import re
text = "hello world"
words = text.split(r"\s+")  # Wrong: Python's split doesn't take regex

# GOOD: Correct Python approach
import re
text = "hello world"
words = re.split(r"\s+", text)
# OR simply:
words = text.split()
```

### Error Severity Classification

- **Single-line errors**: Simple typos or missing symbols
- **Single hunk errors**: Errors confined to one logical block
- **Multi-hunk errors**: Errors spanning multiple sections (40%+ of LLM errors)

### Prompt Length Impact

- **Short prompts (<50 words)**: Higher success rates
- **Long prompts**: Increased likelihood of errors, meaningless/garbage code
- **Recommendation**: Break complex tasks into smaller, focused prompts

### Real-World Impact

- **Copilot study**: 41% more errors introduced when using AI assistance
- **Security study**: 40% of GPT-generated code contained vulnerabilities
- **Package hallucination**: Critical supply chain security risk

---

## Code Review Techniques

### Manual Review Checklist

#### 1. Style and Formatting

- [ ] Code follows PEP 8 or project style guide
- [ ] Consistent naming conventions throughout
- [ ] Proper use of whitespace and blank lines
- [ ] Line length within acceptable limits

#### 2. Naming and Clarity

- [ ] Classes, variables, and functions have descriptive names
- [ ] Names accurately reflect purpose and behavior
- [ ] Avoid abbreviations unless universally understood
- [ ] No misleading or confusing names

#### 3. Function Design

- [ ] Functions have a single, well-defined purpose
- [ ] Function length is reasonable (<50 lines as guideline)
- [ ] Parameters are necessary and well-ordered
- [ ] Return values are consistent and documented

#### 4. Design Patterns

- [ ] Design patterns implemented correctly
- [ ] No reinventing standard library functionality
- [ ] Appropriate use of object-oriented principles
- [ ] SOLID principles followed where applicable

#### 5. Type Safety

- [ ] Type hints used appropriately
- [ ] Immutable types preferred where possible
- [ ] No mutable default arguments
- [ ] Proper handling of None values

#### 6. Input Validation

- [ ] All inputs are sanitized and validated
- [ ] Edge cases are handled
- [ ] Error messages are informative
- [ ] No assumptions about input format

#### 7. Code Reuse

- [ ] Duplicated code extracted into functions/methods
- [ ] Standard library used instead of custom implementations
- [ ] Shared logic properly abstracted
- [ ] DRY (Don't Repeat Yourself) principle followed

#### 8. Error Handling

- [ ] Appropriate exceptions raised for error conditions
- [ ] Exceptions are specific, not generic
- [ ] Resources properly cleaned up (using context managers)
- [ ] Error messages provide actionable information

### Automated Review Process

#### Continuous Integration

- Add linting to build validation pipeline
- Run formatters and linters in pre-commit hooks
- Integrate static analysis into CI/CD
- Fail builds on critical issues

#### IDE Integration

- Configure IDE to show PEP 8 violations
- Enable type checking in editor
- Use auto-formatting on save
- Display complexity metrics

---

## Static Analysis Tools

### Core Tools

#### 1. Pylint

**Purpose**: Comprehensive linter checking PEP 8 compliance and code quality

**Features**:
- Checks adherence to PEP 8
- Detects code smells and anti-patterns
- Measures code complexity
- Enforces naming conventions
- More stringent than Flake8

**Usage**:
```bash
pip install pylint
pylint your_module.py
```

**Configuration**: Create `.pylintrc` to customize rules

#### 2. Flake8

**Purpose**: Fast, focused linter for PEP 8 compliance

**Features**:
- Combines PyFlakes, pycodestyle, and McCabe complexity checker
- Lightweight and fast
- Less strict than Pylint
- Extensible with plugins

**Usage**:
```bash
pip install flake8
flake8 your_module.py
```

#### 3. Mypy

**Purpose**: Static type checker for Python

**Features**:
- Validates type hints (PEP 484)
- Catches type-related bugs before runtime
- Gradual typing support (works with partial annotations)
- Detects inconsistent return types

**Usage**:
```bash
pip install mypy
mypy your_module.py
```

**Configuration**: Create `mypy.ini` for project settings

#### 4. Bandit

**Purpose**: Security-focused static analysis

**Features**:
- Identifies security vulnerabilities
- Detects hardcoded passwords and secrets
- Finds SQL injection vulnerabilities
- Checks for unsafe deserialization
- Created by Python Code Quality Authority (PyCQA)

**Usage**:
```bash
pip install bandit
bandit -r your_project/
```

#### 5. Prospector

**Purpose**: Meta-tool combining multiple analyzers

**Features**:
- Runs Pylint, Pyflakes, McCabe, and more
- Intelligent output filtering
- Suppresses nagging warnings
- Provides unified, actionable results

**Usage**:
```bash
pip install prospector
prospector your_project/
```

### Additional Tools

#### Black (Formatter)

**Purpose**: Opinionated code formatter

**Features**:
- Zero configuration required
- Deterministic formatting
- Enforces consistent style
- Integration with most editors

**Usage**:
```bash
pip install black
black your_module.py
```

#### isort (Import Organizer)

**Purpose**: Automatically organise imports

**Features**:
- Sorts imports alphabetically
- Groups by standard library, third-party, local
- Configurable grouping rules

**Usage**:
```bash
pip install isort
isort your_module.py
```

#### Ruff (Modern All-in-One)

**Purpose**: Fast Python linter written in Rust

**Features**:
- 10-100x faster than existing linters
- Replaces Flake8, isort, and more
- Auto-fixes many issues
- Growing adoption in 2024-2025

**Usage**:
```bash
pip install ruff
ruff check your_module.py
ruff format your_module.py
```

### Recommended Tool Stack

**Minimal Setup**:
- Black (formatting)
- Flake8 or Ruff (linting)
- Mypy (type checking)

**Comprehensive Setup**:
- Black (formatting)
- Ruff (linting + import sorting)
- Mypy (type checking)
- Bandit (security)
- Pytest (testing)

---

## Python Anti-Patterns

Anti-patterns are common solutions to recurring problems that are ineffective and counterproductive.

### 1. Empty or Broad Exception Blocks

**Problem**: Catches and hides all errors, making debugging nearly impossible

```python
# BAD: Silent failure
try:
    risky_operation()
except:
    pass

# BAD: Too broad
try:
    risky_operation()
except Exception:
    pass

# GOOD: Specific exception handling
try:
    risky_operation()
except ValueError as e:
    logger.error(f"Invalid value: {e}")
    raise
except IOError as e:
    logger.error(f"IO error: {e}")
    # Handle or re-raise as appropriate
```

### 2. Wildcard Imports

**Problem**: Pollutes namespace, causes name clashes, reduces clarity

```python
# BAD: Wildcard import
from module import *

# GOOD: Explicit imports
from module import function_one, function_two, ClassOne

# ALSO GOOD: Namespace import
import module
module.function_one()
```

### 3. Inconsistent Return Types

**Problem**: Makes code confusing and error-prone

```python
# BAD: Inconsistent returns
def find_user(user_id):
    if user_id in database:
        return database[user_id]
    else:
        return False  # Should return None, not False

# GOOD: Consistent return types
def find_user(user_id) -> Optional[User]:
    if user_id in database:
        return database[user_id]
    return None
```

### 4. Not Using Context Managers

**Problem**: Resources may not be properly released, especially on exceptions

```python
# BAD: Manual resource management
f = open("file.txt", "r")
data = f.read()
f.close()  # Might not be called if exception occurs

# GOOD: Context manager ensures cleanup
with open("file.txt", "r") as f:
    data = f.read()
# File automatically closed even if exception occurs
```

### 5. Unnecessary Comprehensions

**Problem**: Creating intermediate lists when generator expressions suffice

```python
# BAD: Unnecessary list comprehension
if any([x > 10 for x in numbers]):
    pass

# GOOD: Generator expression (more efficient)
if any(x > 10 for x in numbers):
    pass

# BAD: Building list just to iterate
total = sum([x * 2 for x in range(1000000)])

# GOOD: Generator expression (saves memory)
total = sum(x * 2 for x in range(1000000))
```

### 6. Mutable Default Arguments

**Problem**: Default mutable objects are shared across function calls

```python
# BAD: Mutable default argument
def add_item(item, items=[]):
    items.append(item)
    return items

# Each call shares the same list!
print(add_item(1))  # [1]
print(add_item(2))  # [1, 2] - Unexpected!

# GOOD: Use None and create new instance
def add_item(item, items=None):
    if items is None:
        items = []
    items.append(item)
    return items
```

### 7. Using `is` for Value Comparison

**Problem**: `is` checks identity, not equality

```python
# BAD: Using 'is' for value comparison
if name is "John":  # Wrong!
    pass

# GOOD: Use == for value comparison
if name == "John":
    pass

# CORRECT use of 'is': Check for None, True, False
if value is None:
    pass
```

### 8. Not Following EAFP

**EAFP**: "Easier to Ask for Forgiveness than Permission" - a Python principle

```python
# BAD: LBYL (Look Before You Leap) - not Pythonic
if key in dictionary:
    value = dictionary[key]
else:
    value = default_value

# GOOD: EAFP - more Pythonic
try:
    value = dictionary[key]
except KeyError:
    value = default_value

# BEST: Use built-in method
value = dictionary.get(key, default_value)
```

### 9. String Concatenation in Loops

**Problem**: Creates new string object each iteration (inefficient)

```python
# BAD: Repeated string concatenation
result = ""
for item in items:
    result += str(item) + ", "

# GOOD: Use join()
result = ", ".join(str(item) for item in items)
```

### 10. Ignoring Pythonic Idioms

```python
# BAD: Manual index tracking
i = 0
for item in items:
    print(i, item)
    i += 1

# GOOD: Use enumerate
for i, item in enumerate(items):
    print(i, item)

# BAD: Building list of keys
keys = []
for key in dictionary:
    keys.append(key)

# GOOD: Direct conversion
keys = list(dictionary)  # or list(dictionary.keys())
```

---

## Security Vulnerabilities

Security is critical in Python applications. Common vulnerabilities align with OWASP Top 10.

### 1. Injection Vulnerabilities

#### SQL Injection

**Problem**: Unsanitized input in SQL queries

```python
# CRITICAL VULNERABILITY: SQL Injection
user_input = request.GET['username']
query = f"SELECT * FROM users WHERE username = '{user_input}'"
cursor.execute(query)  # NEVER DO THIS

# SECURE: Parameterized queries
user_input = request.GET['username']
query = "SELECT * FROM users WHERE username = %s"
cursor.execute(query, (user_input,))

# SECURE: ORM usage (e.g., SQLAlchemy)
user = session.query(User).filter(User.username == user_input).first()
```

#### Command Injection

**Problem**: Passing user input to shell commands

```python
# CRITICAL VULNERABILITY: Command Injection
filename = request.GET['file']
os.system(f"cat {filename}")  # NEVER DO THIS

# SECURE: Use subprocess with list arguments
import subprocess
filename = request.GET['file']
# Validate filename first!
if not is_valid_filename(filename):
    raise ValueError("Invalid filename")
subprocess.run(["cat", filename], check=True)

# BETTER: Avoid shell commands entirely
with open(filename, 'r') as f:
    content = f.read()
```

### 2. XML External Entity (XXE) Attacks

**Problem**: Malformed XML can execute arbitrary code or access files

```python
# VULNERABLE: Default XML parsing
import xml.etree.ElementTree as ET
tree = ET.parse(untrusted_xml_file)

# SECURE: Disable entity processing (Python 3.9.1+)
import defusedxml.ElementTree as ET
tree = ET.parse(untrusted_xml_file)
```

### 3. Deserialization Vulnerabilities

**Problem**: Unpickling untrusted data can execute arbitrary code

```python
# CRITICAL VULNERABILITY: Unsafe deserialization
import pickle
with open('untrusted_data.pkl', 'rb') as f:
    data = pickle.load(f)  # Can execute arbitrary code!

# SECURE: Use JSON or other safe formats
import json
with open('data.json', 'r') as f:
    data = json.load(f)

# If you must use pickle, validate source and use signing
import hmac
import hashlib

def safe_pickle_load(filename, secret_key):
    with open(filename, 'rb') as f:
        signature = f.read(32)
        data = f.read()
        expected_sig = hmac.new(secret_key, data, hashlib.sha256).digest()
        if not hmac.compare_digest(signature, expected_sig):
            raise ValueError("Invalid signature")
        return pickle.loads(data)
```

### 4. Path Traversal

**Problem**: User input allows access to files outside intended directory

```python
# VULNERABLE: Path traversal
import os
filename = request.GET['file']
with open(f"/var/data/{filename}", 'r') as f:  # User could pass "../../etc/passwd"
    content = f.read()

# SECURE: Validate and normalize paths
import os
from pathlib import Path

def safe_file_read(filename):
    base_dir = Path("/var/data").resolve()
    file_path = (base_dir / filename).resolve()

    # Ensure file is within base directory
    if not str(file_path).startswith(str(base_dir)):
        raise ValueError("Invalid file path")

    with open(file_path, 'r') as f:
        return f.read()
```

### 5. Hardcoded Secrets

**Problem**: Credentials in source code

```python
# CRITICAL VULNERABILITY: Hardcoded credentials
API_KEY = "sk_live_a1b2c3d4e5f6"  # NEVER DO THIS
db_password = "super_secret_123"

# SECURE: Use environment variables
import os
API_KEY = os.environ.get("API_KEY")
if not API_KEY:
    raise ValueError("API_KEY environment variable not set")

# SECURE: Use dedicated secrets management
from secretsmanager import get_secret
db_password = get_secret("database/password")
```

### 6. Insecure Randomness

**Problem**: Using predictable random numbers for security purposes

```python
# INSECURE: Predictable random
import random
token = random.randint(1000000, 9999999)  # NOT cryptographically secure

# SECURE: Cryptographically secure random
import secrets
token = secrets.token_urlsafe(32)
session_id = secrets.token_hex(16)
reset_code = secrets.randbelow(1000000)
```

### 7. Weak Cryptography

**Problem**: Using outdated or weak cryptographic methods

```python
# INSECURE: MD5 or SHA1 for passwords
import hashlib
password_hash = hashlib.md5(password.encode()).hexdigest()  # NEVER DO THIS

# SECURE: Use bcrypt, scrypt, or argon2
import bcrypt
password_hash = bcrypt.hashpw(password.encode(), bcrypt.gensalt())

# To verify:
if bcrypt.checkpw(input_password.encode(), password_hash):
    # Password correct
    pass
```

### 8. Denial of Service (DoS)

**Problem**: CPU/RAM exhaustion from malformed input

```python
# VULNERABLE: Unbounded resource consumption
import xml.etree.ElementTree as ET
tree = ET.parse(untrusted_xml)  # Billion laughs attack possible

# SECURE: Set limits and timeouts
from defusedxml import ElementTree as ET
# defusedxml provides protection against various XML attacks

# ALSO: Limit input sizes
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10 MB
if len(request_data) > MAX_FILE_SIZE:
    raise ValueError("File too large")
```

### 9. Insecure Dependencies

**Problem**: Using packages with known vulnerabilities

```python
# VULNERABLE: Outdated dependencies
# requirements.txt:
# requests==2.0.0  # Has known vulnerabilities

# SECURE: Keep dependencies updated
# requirements.txt:
# requests>=2.31.0

# Use tools to check for vulnerabilities:
# pip install safety
# safety check
# OR
# pip install pip-audit
# pip-audit
```

### 10. Insufficient Logging and Monitoring

**Problem**: Security events not logged or monitored

```python
# BAD: No logging of security events
def login(username, password):
    if check_credentials(username, password):
        return create_session()
    return None

# GOOD: Log security-relevant events
import logging

def login(username, password):
    if check_credentials(username, password):
        logging.info(f"Successful login for user: {username}")
        return create_session()
    else:
        logging.warning(f"Failed login attempt for user: {username}")
        return None
```

---

## Testing Best Practices

### pytest Framework

pytest is the de facto standard for Python testing.

#### Basic Test Structure

```python
# test_calculator.py
import pytest
from calculator import add, divide

def test_add():
    assert add(2, 3) == 5
    assert add(-1, 1) == 0
    assert add(0, 0) == 0

def test_divide():
    assert divide(10, 2) == 5
    assert divide(9, 3) == 3
```

#### Testing Exceptions

```python
import pytest

def test_divide_by_zero():
    with pytest.raises(ValueError):
        divide(10, 0)

def test_divide_by_zero_with_message():
    with pytest.raises(ValueError, match="Cannot divide by zero"):
        divide(10, 0)

# Access exception details
def test_exception_details():
    with pytest.raises(ValueError) as exc_info:
        divide(10, 0)
    assert "divide" in str(exc_info.value).lower()
```

#### Fixtures for Setup/Teardown

```python
import pytest

@pytest.fixture
def sample_data():
    """Provide test data."""
    return [1, 2, 3, 4, 5]

@pytest.fixture
def database_connection():
    """Setup and teardown for database."""
    conn = create_connection()
    yield conn  # Test runs here
    conn.close()  # Cleanup after test

def test_with_fixture(sample_data):
    assert len(sample_data) == 5
    assert sum(sample_data) == 15
```

#### Parametrized Tests

```python
@pytest.mark.parametrize("input,expected", [
    (2, 4),
    (3, 9),
    (4, 16),
    (5, 25),
])
def test_square(input, expected):
    assert square(input) == expected
```

#### Testing Custom Exceptions

```python
class InsufficientFundsError(Exception):
    """Raised when account has insufficient funds."""
    pass

def test_custom_exception():
    with pytest.raises(InsufficientFundsError):
        account.withdraw(1000000)
```

### Test Organisation Best Practices

#### 1. Test Structure

- **Arrange-Act-Assert (AAA) Pattern**:
  ```python
  def test_user_creation():
      # Arrange
      username = "john_doe"
      email = "john@example.com"

      # Act
      user = create_user(username, email)

      # Assert
      assert user.username == username
      assert user.email == email
  ```

#### 2. Test Independence

- Each test should be independent
- Tests should not rely on execution order
- Use fixtures for shared setup

#### 3. Test Coverage

- Aim for high coverage (>80%) but don't chase 100%
- Focus on critical paths and edge cases
- Use `pytest-cov` to measure coverage:
  ```bash
  pip install pytest-cov
  pytest --cov=myproject tests/
  ```

#### 4. Testing Best Practices

- **Test one thing per test**: Keep tests focused
- **Use descriptive test names**: `test_user_cannot_withdraw_more_than_balance()`
- **Test edge cases**: Empty lists, None values, boundary conditions
- **Test error conditions**: Ensure proper exception handling
- **Mock external dependencies**: Don't hit real APIs or databases in unit tests

#### 5. Mocking External Dependencies

```python
from unittest.mock import Mock, patch

def test_api_call():
    with patch('requests.get') as mock_get:
        mock_get.return_value.json.return_value = {'data': 'test'}

        result = fetch_data_from_api()

        assert result == {'data': 'test'}
        mock_get.assert_called_once()
```

### Common Testing Anti-Patterns

```python
# BAD: Testing implementation details
def test_internal_method():
    obj = MyClass()
    assert obj._internal_helper() == 5  # Don't test private methods

# GOOD: Test public interface
def test_public_behavior():
    obj = MyClass()
    assert obj.calculate() == 5

# BAD: Over-mocking
def test_with_too_many_mocks():
    with patch('module1.func1'), patch('module2.func2'), \
         patch('module3.func3'), patch('module4.func4'):
        # If you need this many mocks, your code might be too coupled
        pass

# GOOD: Test with real objects when possible
def test_with_real_objects():
    obj = MyClass()
    result = obj.process(real_data)
    assert result == expected
```

---

## Performance Optimization

### Common Performance Pitfalls

#### 1. Inefficient String Concatenation

**Problem**: String concatenation in loops creates new objects each iteration

```python
# BAD: O(n²) complexity
result = ""
for i in range(10000):
    result += str(i) + ", "

# GOOD: O(n) complexity
result = ", ".join(str(i) for i in range(10000))
```

**Impact**: For 10,000 iterations, join() is 10-100x faster.

#### 2. Wrong Data Structure Choice

**Problem**: Using lists when sets/dicts would be more efficient

```python
# BAD: O(n) lookup time
items = [1, 2, 3, 4, 5, ...]  # List with thousands of items
if target in items:  # Scans entire list
    pass

# GOOD: O(1) average lookup time
items = {1, 2, 3, 4, 5, ...}  # Set
if target in items:  # Instant lookup
    pass

# BAD: Searching list for matches
users = [{'id': 1, 'name': 'Alice'}, {'id': 2, 'name': 'Bob'}, ...]
user = next(u for u in users if u['id'] == target_id)  # O(n)

# GOOD: Dictionary for O(1) lookups
users = {1: {'id': 1, 'name': 'Alice'}, 2: {'id': 2, 'name': 'Bob'}, ...}
user = users[target_id]  # O(1)
```

#### 3. Not Using List Comprehensions

**Problem**: Traditional loops are slower than comprehensions

```python
# BAD: Slower traditional loop
squares = []
for i in range(1000):
    squares.append(i * i)

# GOOD: Faster list comprehension
squares = [i * i for i in range(1000)]

# EVEN BETTER: Generator for large datasets (saves memory)
squares = (i * i for i in range(1000000))
```

#### 4. Not Using Generators

**Problem**: Building entire list when only iterating once

```python
# BAD: Builds entire list in memory (8MB for 1M integers)
def get_numbers(n):
    return [i for i in range(n)]

for num in get_numbers(1000000):
    process(num)

# GOOD: Generator (minimal memory)
def get_numbers(n):
    for i in range(n):
        yield i

for num in get_numbers(1000000):
    process(num)

# ALSO GOOD: Generator expression
for num in (i for i in range(1000000)):
    process(num)
```

#### 5. Unnecessary Function Calls in Loops

**Problem**: Calling functions repeatedly instead of caching results

```python
# BAD: Calls len() every iteration
for i in range(len(items)):
    if i < len(items) - 1:  # len() called again!
        pass

# GOOD: Cache length
length = len(items)
for i in range(length):
    if i < length - 1:
        pass

# BEST: Avoid manual indexing entirely
for i, item in enumerate(items):
    if i < len(items) - 1:
        pass
```

#### 6. Not Using Built-in Functions

**Problem**: Python's built-ins are optimized in C

```python
# BAD: Manual implementation
def my_sum(numbers):
    total = 0
    for num in numbers:
        total += num
    return total

# GOOD: Use built-in (much faster)
total = sum(numbers)

# BAD: Manual min/max
minimum = numbers[0]
for num in numbers:
    if num < minimum:
        minimum = num

# GOOD: Use built-in
minimum = min(numbers)
```

#### 7. Mutable Default Arguments (Performance)

**Problem**: Beyond correctness issues, impacts performance

```python
# BAD: Shared mutable default (also a correctness issue)
def add_to_cache(item, cache=[]):
    cache.append(item)
    return cache

# GOOD: Create new instance each time
def add_to_cache(item, cache=None):
    if cache is None:
        cache = []
    cache.append(item)
    return cache
```

### Optimization Best Practices

#### 1. Profile Before Optimizing

**Principle**: "Premature optimization is the root of all evil" - Donald Knuth

```python
# Use cProfile for profiling
import cProfile
import pstats

cProfile.run('my_function()', 'profile_stats')
p = pstats.Stats('profile_stats')
p.sort_stats('cumulative').print_stats(10)

# Or use line_profiler for line-by-line analysis
# pip install line_profiler
# kernprof -l -v my_script.py
```

#### 2. Use Caching/Memoization

```python
from functools import lru_cache

# Expensive computation with caching
@lru_cache(maxsize=128)
def fibonacci(n):
    if n < 2:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Without cache: fib(35) takes ~5 seconds
# With cache: fib(35) takes microseconds
```

#### 3. Use Appropriate Algorithms

**Problem**: O(n²) when O(n log n) or O(n) exists

```python
# BAD: O(n²) nested loops to find duplicates
duplicates = []
for i, item in enumerate(items):
    for j, other in enumerate(items):
        if i != j and item == other:
            duplicates.append(item)

# GOOD: O(n) using set
seen = set()
duplicates = []
for item in items:
    if item in seen:
        duplicates.append(item)
    seen.add(item)

# EVEN BETTER: Using Counter
from collections import Counter
duplicates = [item for item, count in Counter(items).items() if count > 1]
```

#### 4. Local Variable Lookups Are Faster

```python
# SLOWER: Global lookup
import math

def calculate(x):
    return math.sqrt(x) * math.pi

# FASTER: Local lookup
import math

def calculate(x):
    sqrt = math.sqrt
    pi = math.pi
    return sqrt(x) * pi
```

#### 5. Use slots for Classes with Many Instances

```python
# Without __slots__: Each instance has __dict__ (overhead)
class Point:
    def __init__(self, x, y):
        self.x = x
        self.y = y

# With __slots__: Faster and uses less memory
class Point:
    __slots__ = ['x', 'y']

    def __init__(self, x, y):
        self.x = x
        self.y = y

# Benchmark: 1M instances
# Without slots: ~180 MB
# With slots: ~60 MB (66% reduction)
```

### Performance Measurement

```python
# Simple timing
import time

start = time.time()
expensive_operation()
end = time.time()
print(f"Time: {end - start:.4f} seconds")

# More accurate timing with timeit
import timeit

time = timeit.timeit('my_function()', setup='from __main__ import my_function', number=1000)
print(f"Average time: {time/1000:.6f} seconds")
```

---

## Concurrency & Async Patterns

### Understanding Python Concurrency

Python offers three main approaches to concurrency:

1. **Threading**: Good for I/O-bound tasks
2. **Multiprocessing**: Good for CPU-bound tasks
3. **Asyncio**: Good for I/O-bound tasks with many concurrent operations

### Common Asyncio Mistakes

#### 1. Calling Coroutines Without await

**Problem**: Coroutines must be awaited or run in event loop

```python
# BAD: Calling coroutine directly
async def fetch_data():
    return await get_api_data()

data = fetch_data()  # Returns coroutine object, doesn't execute!

# GOOD: Await the coroutine
async def main():
    data = await fetch_data()  # Properly executes

# GOOD: Run with asyncio.run (Python 3.7+)
import asyncio
data = asyncio.run(fetch_data())
```

#### 2. Not Letting Coroutines Run in Event Loop

**Problem**: Creating coroutines but never scheduling them

```python
# BAD: Coroutine created but never scheduled
async def process_item(item):
    await api_call(item)

async def main():
    items = [1, 2, 3, 4, 5]
    for item in items:
        process_item(item)  # Missing await!

# GOOD: Await each coroutine
async def main():
    items = [1, 2, 3, 4, 5]
    for item in items:
        await process_item(item)

# BETTER: Run concurrently with gather
async def main():
    items = [1, 2, 3, 4, 5]
    await asyncio.gather(*[process_item(item) for item in items])
```

#### 3. Blocking the Event Loop

**Problem**: CPU-bound operations block all coroutines

```python
# BAD: Blocking operation in async function
import time

async def bad_async():
    time.sleep(5)  # Blocks entire event loop!
    return "Done"

# GOOD: Use asyncio.sleep for I/O
async def good_async():
    await asyncio.sleep(5)  # Allows other coroutines to run
    return "Done"

# GOOD: Offload CPU-bound work to executor
import asyncio
from concurrent.futures import ProcessPoolExecutor

def cpu_intensive_task(data):
    # Expensive computation
    return result

async def main():
    loop = asyncio.get_event_loop()
    with ProcessPoolExecutor() as executor:
        result = await loop.run_in_executor(executor, cpu_intensive_task, data)
```

#### 4. Using await Too Much

**Problem**: Sequential awaits when parallel execution possible

```python
# BAD: Sequential execution (10 seconds total)
async def fetch_all_data():
    data1 = await fetch_api_1()  # 5 seconds
    data2 = await fetch_api_2()  # 5 seconds
    return data1, data2

# GOOD: Concurrent execution (5 seconds total)
async def fetch_all_data():
    data1, data2 = await asyncio.gather(
        fetch_api_1(),
        fetch_api_2()
    )
    return data1, data2
```

#### 5. Mixing Threads and Asyncio

**Problem**: Asyncio objects are not thread-safe

```python
# BAD: Calling asyncio from thread
import threading
import asyncio

def thread_worker():
    result = asyncio.run(async_function())  # Creates new event loop!

# GOOD: Use asyncio.run_coroutine_threadsafe
async def async_function():
    return "result"

def thread_worker(loop):
    future = asyncio.run_coroutine_threadsafe(async_function(), loop)
    result = future.result()

# In main thread
loop = asyncio.get_event_loop()
thread = threading.Thread(target=thread_worker, args=(loop,))
thread.start()
```

#### 6. Not Handling Task Cancellation

**Problem**: Tasks can be cancelled, must handle gracefully

```python
# BAD: No cancellation handling
async def worker():
    await asyncio.sleep(100)
    cleanup()  # Never called if cancelled

# GOOD: Handle cancellation
async def worker():
    try:
        await asyncio.sleep(100)
    except asyncio.CancelledError:
        cleanup()
        raise  # Re-raise to signal cancellation
    finally:
        # Always runs
        final_cleanup()
```

### Threading Best Practices

#### 1. Understanding the GIL

**The Global Interpreter Lock (GIL)** prevents true parallelism for CPU-bound tasks in Python.

- **Threading**: Good for I/O-bound (network, disk)
- **Multiprocessing**: Required for CPU-bound (calculations)

```python
# Threading for I/O-bound tasks
import threading
import requests

def fetch_url(url):
    response = requests.get(url)
    return response.text

urls = ["http://example.com", "http://example.org", ...]
threads = [threading.Thread(target=fetch_url, args=(url,)) for url in urls]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()

# Multiprocessing for CPU-bound tasks
from multiprocessing import Pool

def expensive_calculation(n):
    return sum(i*i for i in range(n))

with Pool(4) as p:
    results = p.map(expensive_calculation, [10000000, 20000000, 30000000])
```

#### 2. Thread Safety

**Problem**: Shared state without synchronization

```python
# BAD: Race condition
counter = 0

def increment():
    global counter
    for _ in range(100000):
        counter += 1  # Not atomic!

threads = [threading.Thread(target=increment) for _ in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()
print(counter)  # Will be less than 1,000,000!

# GOOD: Use locks
import threading

counter = 0
lock = threading.Lock()

def increment():
    global counter
    for _ in range(100000):
        with lock:
            counter += 1

threads = [threading.Thread(target=increment) for _ in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()
print(counter)  # Will be exactly 1,000,000
```

### When to Use Each Approach

```python
# I/O-bound with many concurrent operations: asyncio
async def fetch_many_urls(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        return await asyncio.gather(*tasks)

# I/O-bound with traditional libraries: threading
from concurrent.futures import ThreadPoolExecutor

def fetch_many_urls(urls):
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(requests.get, urls)
    return list(results)

# CPU-bound: multiprocessing
from concurrent.futures import ProcessPoolExecutor

def process_data(data_chunks):
    with ProcessPoolExecutor() as executor:
        results = executor.map(expensive_computation, data_chunks)
    return list(results)
```

---

## Documentation Standards

### PEP 257 - Docstring Conventions

#### Basic Docstring Format

```python
def function(arg1, arg2):
    """
    Summary line (one line, ends with period).

    Extended description of function (optional).
    Can span multiple lines and paragraphs.

    Args:
        arg1: Description of arg1
        arg2: Description of arg2

    Returns:
        Description of return value

    Raises:
        ValueError: When input is invalid
        TypeError: When wrong type is provided
    """
    pass
```

#### Module Docstrings

```python
"""
Module name and one-line summary.

Extended module description. This module provides functionality for X, Y, and Z.
It includes the following main components:
- ComponentA: Does X
- ComponentB: Does Y

Example:
    Basic usage example::

        from mymodule import myfunction
        result = myfunction(arg)
"""

import os
import sys
```

#### Class Docstrings

```python
class MyClass:
    """
    One-line summary of the class.

    Extended description of the class purpose and behavior.

    Attributes:
        attribute1 (str): Description of attribute1
        attribute2 (int): Description of attribute2
    """

    def __init__(self, param1, param2):
        """
        Initialize MyClass.

        Args:
            param1 (str): Description of param1
            param2 (int): Description of param2
        """
        self.attribute1 = param1
        self.attribute2 = param2

    def method(self, arg):
        """
        Summary of method behavior.

        Args:
            arg (str): Description of argument

        Returns:
            bool: True if successful, False otherwise
        """
        pass
```

#### One-Line Docstrings

```python
def simple_function():
    """Do something simple and return a value."""
    return 42

class SimpleClass:
    """A simple class for demonstration purposes."""
    pass
```

### Popular Docstring Formats

#### Google Style

```python
def function(arg1, arg2):
    """
    Summary of function.

    Detailed description if needed.

    Args:
        arg1 (str): The first parameter.
        arg2 (int): The second parameter.

    Returns:
        bool: The return value. True for success, False otherwise.

    Raises:
        ValueError: If arg1 is empty.
        TypeError: If arg2 is not an integer.
    """
    pass
```

#### NumPy Style

```python
def function(arg1, arg2):
    """
    Summary of function.

    Detailed description if needed.

    Parameters
    ----------
    arg1 : str
        The first parameter.
    arg2 : int
        The second parameter.

    Returns
    -------
    bool
        The return value. True for success, False otherwise.

    Raises
    ------
    ValueError
        If arg1 is empty.
    TypeError
        If arg2 is not an integer.
    """
    pass
```

#### Sphinx/reStructuredText Style

```python
def function(arg1, arg2):
    """
    Summary of function.

    Detailed description if needed.

    :param arg1: The first parameter
    :type arg1: str
    :param arg2: The second parameter
    :type arg2: int
    :return: The return value. True for success, False otherwise
    :rtype: bool
    :raises ValueError: If arg1 is empty
    :raises TypeError: If arg2 is not an integer
    """
    pass
```

### Documentation Best Practices

1. **Always use `"""triple double quotes"""`** - Never single quotes for docstrings
2. **First line should be a one-line summary** - Ending with a period
3. **Document all public modules, functions, classes, and methods**
4. **Describe behavior, not implementation** - Focus on what, not how
5. **Include examples for complex functionality**
6. **Document exceptions that can be raised**
7. **Keep docstrings up to date** - Outdated docs are worse than no docs
8. **Use consistent style within project** - Pick one format and stick to it

---

## Database & Type System Pitfalls

**Real-world lessons from GeistFabrik implementation**

This section covers common issues when working with databases and type systems in Python, particularly when using SQLite with NumPy/pandas.

### SQLite Type System Quirks

SQLite has a dynamic type system that can cause unexpected behavior with Python type annotations and NumPy types.

#### NumPy Type Serialization

**Problem**: NumPy integer and float types are not recognized as native Python types by SQLite's parameter binding.

```python
import numpy as np
import sqlite3

# BAD: NumPy types serialized as blobs
conn = sqlite3.connect("data.db")
conn.execute("CREATE TABLE metrics (id INTEGER, count INTEGER)")

n_items = np.int64(42)  # From NumPy computation
conn.execute("INSERT INTO metrics VALUES (1, ?)", (n_items,))
conn.commit()

# Check what was stored
cursor = conn.execute("SELECT typeof(count), count FROM metrics")
print(cursor.fetchone())
# Output: ('blob', b'*\x00\x00\x00\x00\x00\x00\x00')
# ❌ Stored as binary blob, not INTEGER!

# GOOD: Convert NumPy types to Python types
def to_python_type(val: Any) -> Any:
    """Convert NumPy types to native Python types for SQLite."""
    if val is None:
        return None
    if isinstance(val, (np.integer, np.int8, np.int16, np.int32, np.int64)):
        return int(val)
    if isinstance(val, (np.floating, np.float16, np.float32, np.float64)):
        return float(val)
    if isinstance(val, np.ndarray):
        return val.tolist()
    return val

n_items = to_python_type(np.int64(42))
conn.execute("INSERT INTO metrics VALUES (2, ?)", (n_items,))
conn.commit()

# Verify correct storage
cursor = conn.execute("SELECT typeof(count), count FROM metrics WHERE id = 2")
print(cursor.fetchone())
# Output: ('integer', 42)
# ✅ Correctly stored as INTEGER!
```

#### Reading Blob Data

**Problem**: Existing databases may have blob data that needs to be converted back.

```python
import struct

def load_from_database(cursor_row):
    """Load data from database, handling both blobs and native types."""
    value = cursor_row[0]

    # Handle blob deserialization
    if isinstance(value, bytes):
        try:
            # Assume little-endian int64 (8 bytes)
            if len(value) == 8:
                return struct.unpack('<q', value)[0]
            # Assume little-endian float64 (8 bytes)
            return struct.unpack('<d', value)[0]
        except struct.error:
            return None

    # Handle native types
    return value
```

#### Detection Strategy

```bash
# Check SQLite types in your database
sqlite3 your_database.db <<EOF
SELECT
    name,
    typeof(column_name) as type,
    column_name
FROM your_table
LIMIT 5;
EOF

# Look for unexpected 'blob' types where you expect 'integer' or 'real'
```

### Pandas DataFrame to SQLite

**Problem**: Pandas DataFrames can also cause type issues when writing to SQLite.

```python
import pandas as pd
import sqlite3

# BAD: Direct write without type conversion
df = pd.DataFrame({
    'id': np.array([1, 2, 3], dtype=np.int64),
    'value': np.array([1.5, 2.5, 3.5], dtype=np.float64)
})

conn = sqlite3.connect("data.db")
df.to_sql('data', conn, if_exists='replace', index=False)

# Check types
cursor = conn.execute("SELECT typeof(id), typeof(value) FROM data LIMIT 1")
print(cursor.fetchone())
# May show: ('blob', 'blob') ❌

# GOOD: Convert dtypes before writing
df_converted = df.copy()
for col in df_converted.select_dtypes(include=[np.integer]).columns:
    df_converted[col] = df_converted[col].astype(int)
for col in df_converted.select_dtypes(include=[np.floating]).columns:
    df_converted[col] = df_converted[col].astype(float)

df_converted.to_sql('data', conn, if_exists='replace', index=False, dtype={
    'id': 'INTEGER',
    'value': 'REAL'
})
```

### Type Hints vs Runtime Types

**Problem**: Type hints don't enforce runtime type checking.

```python
# BAD: Type hint doesn't prevent wrong types
def compute_average(numbers: List[int]) -> float:
    return sum(numbers) / len(numbers)

# This passes type checking but fails at runtime
import numpy as np
data = np.array([1, 2, 3], dtype=np.int64)
result = compute_average(data)  # TypeError: 'numpy.ndarray' object is not iterable
# ❌ Type hint didn't help

# GOOD: Validate and convert at runtime
from typing import Union, List
import numpy as np

def compute_average(numbers: Union[List[int], np.ndarray]) -> float:
    """Compute average, accepting both lists and numpy arrays."""
    if isinstance(numbers, np.ndarray):
        numbers = numbers.tolist()
    if not numbers:
        raise ValueError("Cannot compute average of empty sequence")
    return sum(numbers) / len(numbers)
```

### Cache Consistency Patterns

**Problem**: Cached data structures may not match fresh computation results.

```python
# BAD: Returning cached data without validation
def get_metrics(session_id: str, use_cache: bool = True) -> Dict[str, Any]:
    if use_cache:
        cached = load_cache(session_id)
        if cached:
            return cached  # ❌ May be missing fields!

    # Compute fresh metrics
    metrics = {
        "dimension": 387,
        "n_notes": 100,
        "intrinsic_dim": 15.2,
        "vendi_score": 42.5,
        # ... more fields
    }
    save_cache(session_id, metrics)
    return metrics

# GOOD: Always ensure consistent structure
def get_metrics(session_id: str, embeddings: np.ndarray, use_cache: bool = True) -> Dict[str, Any]:
    if use_cache:
        cached = load_cache(session_id)
        if cached:
            # Always populate fields that can change
            cached["dimension"] = embeddings.shape[1]
            cached["n_notes"] = len(embeddings)
            cached["timestamp"] = datetime.now().isoformat()
            return cached

    # Compute fresh metrics
    metrics = compute_all_metrics(embeddings)
    # Cache only expensive computations, not trivial fields
    cacheable = {k: v for k, v in metrics.items()
                 if k not in ["dimension", "n_notes", "timestamp"]}
    save_cache(session_id, cacheable)
    return metrics
```

### Schema Validation

**Problem**: Database schema doesn't match code expectations.

```python
# GOOD: Validate schema on startup
def validate_database_schema(conn: sqlite3.Connection) -> None:
    """Validate that database schema matches expectations."""
    cursor = conn.execute("SELECT sql FROM sqlite_master WHERE type='table' AND name='metrics'")
    schema = cursor.fetchone()

    if not schema:
        raise ValueError("Table 'metrics' does not exist")

    schema_sql = schema[0]

    # Check for required columns with correct types
    required_columns = {
        "session_date": "TEXT",
        "intrinsic_dim": "REAL",
        "n_clusters": "INTEGER",
        "n_gaps": "INTEGER",
    }

    for column, expected_type in required_columns.items():
        if column not in schema_sql:
            raise ValueError(f"Missing column: {column}")
        # Note: This is a simple check; consider using PRAGMA table_info for robust validation

    print("✅ Database schema validated")

# Use at application startup
try:
    conn = sqlite3.connect("vault.db")
    validate_database_schema(conn)
except ValueError as e:
    print(f"❌ Schema validation failed: {e}")
    # Run migration or alert user
```

### Common Pitfalls Checklist

- [ ] **NumPy type conversion**: Convert np.int64/np.float64 to int/float before SQLite insertion
- [ ] **Pandas DataFrame types**: Specify dtypes explicitly when using to_sql()
- [ ] **Blob detection**: Check sqlite3 typeof() to find unexpected blobs
- [ ] **Cache consistency**: Always populate dynamic fields even when using cache
- [ ] **Schema validation**: Validate database schema matches code expectations
- [ ] **Type hint reality**: Don't rely on type hints for runtime type safety
- [ ] **NULL handling**: Explicitly handle None values in database operations
- [ ] **Foreign key types**: Ensure JOIN columns have matching types (INTEGER vs TEXT)

---

## Complete Audit Checklist

This comprehensive checklist consolidates all heuristics for auditing Python codebases.

### 1. Code Style & Formatting

- [ ] **PEP 8 Compliance**: Code follows PEP 8 style guide
  - [ ] 4-space indentation (no tabs)
  - [ ] Maximum line length adhered to (79-99 characters)
  - [ ] Proper whitespace around operators
  - [ ] Blank lines used appropriately

- [ ] **Naming Conventions**: Consistent and descriptive names
  - [ ] `lowercase_with_underscores` for functions/methods/variables
  - [ ] `PascalCase` for classes
  - [ ] `UPPERCASE_WITH_UNDERSCORES` for constants
  - [ ] `_leading_underscore` for private/internal
  - [ ] No single-letter names except loop counters

- [ ] **Import Organisation**: Imports properly organised
  - [ ] Grouped: standard library, third-party, local
  - [ ] Alphabetically sorted within groups
  - [ ] No wildcard imports (`from module import *`)
  - [ ] Unused imports removed

### 2. Type Safety & Documentation

- [ ] **Type Hints**: Functions have appropriate type annotations
  - [ ] Parameters have type hints
  - [ ] Return types specified
  - [ ] `Optional[]` used for nullable values
  - [ ] `Union[]` for multiple acceptable types

- [ ] **Docstrings**: All public APIs documented
  - [ ] Module-level docstrings present
  - [ ] Class docstrings describe purpose and attributes
  - [ ] Function docstrings include Args, Returns, Raises
  - [ ] Consistent docstring format throughout project

### 3. Function Design & Logic

- [ ] **Single Responsibility**: Functions have one clear purpose
  - [ ] Function length reasonable (<50 lines guideline)
  - [ ] No deeply nested logic (>3-4 levels)
  - [ ] Complex conditions extracted into named functions

- [ ] **Parameter Design**: Function parameters well-designed
  - [ ] No mutable default arguments
  - [ ] Parameter count reasonable (<5 guideline)
  - [ ] Related parameters grouped logically

- [ ] **Return Consistency**: Return types consistent
  - [ ] Same type returned from all paths
  - [ ] `None` used consistently (not mixing with `False`, `0`, etc.)
  - [ ] No implicit `None` returns when value expected

### 4. Error Handling

- [ ] **Exception Handling**: Proper exception usage
  - [ ] Specific exceptions caught (not bare `except:`)
  - [ ] No silent failures (`except: pass`)
  - [ ] Custom exceptions for domain-specific errors
  - [ ] Resources properly cleaned up (context managers)

- [ ] **Error Messages**: Informative error messages
  - [ ] Error messages describe what went wrong
  - [ ] Error messages include relevant context
  - [ ] No sensitive data in error messages

### 5. LLM-Specific Error Patterns

- [ ] **Semantic Errors**: No logical mistakes
  - [ ] All necessary conditions checked
  - [ ] No incorrect constant values
  - [ ] Mathematical operations correct
  - [ ] No missing steps in algorithms
  - [ ] No infinite loops or recursions without base case
  - [ ] No unnecessary/garbage code

- [ ] **API Usage**: Correct library/API usage
  - [ ] No hallucinated packages or methods
  - [ ] Correct method signatures (parentheses, parameters)
  - [ ] No cross-language confusion (Java vs Python patterns)

- [ ] **Imports**: All dependencies properly imported
  - [ ] No missing import statements
  - [ ] No undefined variables/functions

- [ ] **Input/Output**: Correct I/O handling
  - [ ] Parameter order correct
  - [ ] Output format matches specification
  - [ ] Precision/rounding appropriate

### 6. Security Vulnerabilities

- [ ] **Injection Prevention**: No injection vulnerabilities
  - [ ] SQL queries use parameterization
  - [ ] No shell injection (avoid `os.system()`, use `subprocess` safely)
  - [ ] No eval/exec with untrusted input

- [ ] **Input Validation**: All inputs validated
  - [ ] User input sanitized before use
  - [ ] File paths validated (no path traversal)
  - [ ] Input size limits enforced

- [ ] **Secrets Management**: No hardcoded secrets
  - [ ] No API keys, passwords, or tokens in code
  - [ ] Environment variables or secret managers used
  - [ ] No credentials in logs

- [ ] **Cryptography**: Strong cryptographic practices
  - [ ] No MD5 or SHA1 for password hashing
  - [ ] bcrypt/scrypt/argon2 used for passwords
  - [ ] `secrets` module used for tokens (not `random`)

- [ ] **Deserialization**: Safe deserialization practices
  - [ ] No `pickle.load()` on untrusted data
  - [ ] JSON preferred over pickle
  - [ ] XML parsing uses defusedxml

- [ ] **Dependencies**: Secure dependency management
  - [ ] Dependencies up to date
  - [ ] No known vulnerabilities (checked with `safety` or `pip-audit`)
  - [ ] Minimal dependency footprint

### 7. Performance

- [ ] **Data Structures**: Appropriate data structure choices
  - [ ] Sets/dicts used for membership testing (not lists)
  - [ ] Generators used for large sequences
  - [ ] No building lists just to iterate once

- [ ] **String Operations**: Efficient string handling
  - [ ] `str.join()` used for concatenation (not `+= in loops`)
  - [ ] f-strings for formatting (not `%` or `.format()` unless needed)

- [ ] **Built-ins**: Python built-ins leveraged
  - [ ] Built-in functions used (sum, min, max, any, all)
  - [ ] List/dict/set comprehensions used appropriately
  - [ ] No reimplementing standard library functionality

- [ ] **Caching**: Appropriate use of caching
  - [ ] `@lru_cache` for expensive pure functions
  - [ ] Results cached when appropriate
  - [ ] No redundant computations in loops

- [ ] **Algorithmic Efficiency**: Efficient algorithms
  - [ ] No O(n²) when O(n log n) or O(n) available
  - [ ] No repeated expensive operations
  - [ ] Profile-guided optimization (not premature)

### 8. Concurrency

- [ ] **Asyncio Usage**: Correct async/await patterns
  - [ ] Coroutines properly awaited
  - [ ] `asyncio.gather()` for concurrent operations
  - [ ] No blocking operations in async functions
  - [ ] CPU-bound work offloaded to executors
  - [ ] Task cancellation handled

- [ ] **Threading**: Proper thread safety
  - [ ] Locks used for shared mutable state
  - [ ] No race conditions
  - [ ] Thread-safe data structures used
  - [ ] Threading used for I/O-bound only

- [ ] **Multiprocessing**: Appropriate use
  - [ ] Multiprocessing used for CPU-bound tasks
  - [ ] Data serialization considered
  - [ ] Process pool properly managed

### 9. Testing

- [ ] **Test Coverage**: Adequate test coverage
  - [ ] Critical paths tested
  - [ ] Edge cases tested
  - [ ] Error conditions tested
  - [ ] Coverage >80% (but not blindly chasing 100%)

- [ ] **Test Quality**: Well-designed tests
  - [ ] Tests independent of each other
  - [ ] AAA pattern (Arrange-Act-Assert) followed
  - [ ] One assertion per test (guideline)
  - [ ] Descriptive test names

- [ ] **Test Isolation**: Proper test isolation
  - [ ] External dependencies mocked
  - [ ] No network calls in unit tests
  - [ ] No database calls in unit tests (use in-memory or mocks)
  - [ ] Tests can run in any order

- [ ] **Exception Testing**: Exceptions properly tested
  - [ ] `pytest.raises()` used correctly
  - [ ] Exception messages validated
  - [ ] Custom exceptions tested

### 10. Anti-Patterns

- [ ] **No Wildcard Imports**: Avoid `from module import *`
- [ ] **No Mutable Defaults**: No mutable default arguments
- [ ] **Context Managers**: `with` statement used for resources
- [ ] **EAFP Over LBYL**: "Easier to Ask Forgiveness than Permission"
- [ ] **No Bare Excepts**: Specific exceptions caught
- [ ] **Identity vs Equality**: `is` used only for None/True/False
- [ ] **Pythonic Idioms**: enumerate, zip, itertools used appropriately
- [ ] **No Premature Optimization**: Profile before optimizing

### 11. Code Organisation

- [ ] **Module Structure**: Logical module organisation
  - [ ] Related functionality grouped
  - [ ] Clear separation of concerns
  - [ ] No circular dependencies

- [ ] **Code Duplication**: Minimal code duplication
  - [ ] DRY principle followed
  - [ ] Common logic extracted
  - [ ] Utilities properly abstracted

- [ ] **Complexity**: Manageable complexity
  - [ ] Cyclomatic complexity reasonable
  - [ ] No god objects/functions
  - [ ] Clear abstraction layers

### 12. Logging & Monitoring

- [ ] **Logging**: Appropriate logging
  - [ ] Security events logged
  - [ ] Error conditions logged
  - [ ] Appropriate log levels used
  - [ ] No sensitive data in logs
  - [ ] Structured logging considered

- [ ] **Debugging**: Code is debuggable
  - [ ] Clear variable names aid debugging
  - [ ] Error messages provide context
  - [ ] Stack traces preserved

### 13. Static Analysis Results

- [ ] **Linter Results**: Clean linter output
  - [ ] Flake8/Pylint passes
  - [ ] No disabled warnings without justification

- [ ] **Type Checker**: Mypy passes
  - [ ] No type errors
  - [ ] No unchecked `Any` types where avoidable

- [ ] **Security Scanner**: Bandit passes
  - [ ] No security issues flagged
  - [ ] Any exceptions properly documented

---

## Audit Workflow

### Initial Assessment

1. **Run Automated Tools**
   ```bash
   # Format check
   black --check .

   # Linting
   flake8 .
   # OR
   ruff check .

   # Type checking
   mypy .

   # Security scanning
   bandit -r .

   # Dependency vulnerabilities
   pip-audit
   # OR
   safety check

   # Test coverage
   pytest --cov=. --cov-report=html
   ```

2. **Review Tool Output**
   - Prioritize by severity
   - Group related issues
   - Identify patterns

### Manual Review

1. **Architecture Review**
   - Examine module structure
   - Check separation of concerns
   - Identify potential refactoring

2. **Code Review** (Use checklist above)
   - Review critical paths first
   - Check security-sensitive code carefully
   - Verify error handling
   - Examine LLM-generated code sections closely

3. **Testing Review**
   - Check test coverage report
   - Review test quality
   - Identify missing test cases

### Documentation

1. **Create Issue List**
   - Categorize by severity (Critical/High/Medium/Low)
   - Link to specific code locations
   - Provide fix recommendations

2. **Generate Report**
   - Executive summary
   - Statistics (issues by category)
   - Detailed findings
   - Recommendations

### Prioritization

**Critical (Fix Immediately)**
- Security vulnerabilities
- Data loss risks
- Production-breaking bugs

**High (Fix Soon)**
- Major LLM error patterns
- Missing error handling in critical paths
- Significant performance issues

**Medium (Fix in Sprint)**
- Anti-patterns
- Missing tests for important features
- Code quality issues

**Low (Technical Debt)**
- Style inconsistencies
- Minor optimizations
- Documentation gaps

---

## Conclusion

This document provides a comprehensive framework for auditing Python codebases, with particular emphasis on identifying issues common in LLM-generated code. The heuristics cover:

- **Style & Standards**: PEP 8, type hints, documentation
- **Correctness**: Logic errors, API misuse, completeness
- **Security**: Injection, secrets, cryptography
- **Performance**: Data structures, algorithms, caching
- **Maintainability**: Testing, organisation, clarity

Regular application of these heuristics, combined with automated tooling, ensures high-quality, secure, and maintainable Python code.

### Key Takeaways

1. **Automate what you can**: Use Black, Flake8/Ruff, Mypy, Bandit
2. **Focus manual review**: Prioritize logic, security, and LLM-specific patterns
3. **Test thoroughly**: High coverage of critical paths and edge cases
4. **Document clearly**: Good docs prevent bugs and aid maintenance
5. **Stay updated**: Keep dependencies current and follow Python evolution

### Resources

- **PEP 8**: https://peps.python.org/pep-0008/
- **PEP 484** (Type Hints): https://peps.python.org/pep-0484/
- **PEP 257** (Docstrings): https://peps.python.org/pep-0257/
- **Python Security**: https://python-security.readthedocs.io/
- **OWASP Top 10**: https://owasp.org/www-project-top-ten/
- **pytest Documentation**: https://docs.pytest.org/
- **Mypy Documentation**: https://mypy.readthedocs.io/
- **Real Python**: https://realpython.com/
- **The Little Book of Python Anti-Patterns**: https://docs.quantifiedcode.com/python-anti-patterns/

---

**Document Version**: 1.1
**Last Updated**: 2025-01-30
**Maintained By**: Research compiled from official Python documentation, academic research, industry best practices, and real-world GeistFabrik implementation lessons

**Changelog**:
- **v1.1 (2025-01-30)**: Added "Systematic Audit Methodology" section based on GeistFabrik stats command implementation audit. Added "Database & Type System Pitfalls" section covering NumPy/SQLite type conversion issues, cache consistency patterns, and schema validation.
